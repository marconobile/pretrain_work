root: /storage_common/nobilm/pretrain_paper/PCQM4Mv2Dataset/experiments
run_name: pretrain_run_test
wandb_project: finetune
seed: 7
dataset_seed: 15
device: cpu #cuda:1
mixed_precision: false

use_grokfast: true
# use_ema: true

model_builders:
    - moreGNNLayers #: freeze #tune
    - Heads

# bessel basis
r_max: 15.0
num_basis: 16

# hyperparams
latent_dim: 256
l_max: 2

# --- HEADS --- #
#[
#  - field: optional, default=AtomicDataDict.NODE_FEATURES_KEY; key used to index data obj, the value is input to head;
#  - out_field: to be used from data to compute loss; you will find head_{out_field}
#  - irreps: irreps outputted by NN
#]
fields_to_mask: [hybridization, chirality, is_aromatic, is_in_ring, group, period, bond_type, bond_stereo, is_conjugated, degree, formal_charge, numH]
heads:
    - [graph_features, homo_lumo_gap, 1x0e]
    - [noise_target, 1x1o]
    - [hybridization, 8x0e]
    - [chirality, 16x0e]
    - [is_aromatic, 4x0e]
    - [is_in_ring, 4x0e]
    - [group, 128x0e]
    - [period, 128x0e]
    - [edge_features, bond_type, 8x0e]
    - [edge_features, bond_stereo, 8x0e]
    - [edge_features, is_conjugated, 4x0e]
    - [graph_features, h_donors_head, 16x0e]
    - [graph_features, h_acceptors_head, 19x0e]
    - [edge_features, binned_dists, 128x0e]
    - [degree, 8x0e]
    - [formal_charge, 8x0e]
    - [numH, 8x0e]
    - [graph_features, count_frags, 128x0e]
    - [graph_features, fragmentdsIds_present, 86x0e] # number of classes in /source/utils/rdkit_fragments.py
    - [graph_features, nsafe, 24x0e]

head_wds: 0.1

head_nsafe_eq_has_internal_weights: false
head_nsafe_mlp_latent_dimensions: [64]
head_nsafe_mlp_nonlinearity: swiglu
head_nsafe_has_bias: true
head_nsafe_gain: 1.55
head_nsafe_dropout: 0.2
head_nsafe_wd: true

head_homo_lumo_gap_eq_has_internal_weights: false
head_homo_lumo_gap_mlp_latent_dimensions: [64]
head_homo_lumo_gap_mlp_nonlinearity: swiglu
head_homo_lumo_gap_has_bias: true
head_homo_lumo_gap_gain: 1.55
head_homo_lumo_gap_dropout: 0.2
head_homo_lumo_gap_wd: true

head_h_donors_head_eq_has_internal_weights: false
head_h_donors_head_mlp_latent_dimensions: [64]
head_h_donors_head_mlp_nonlinearity: swiglu
head_h_donors_head_has_bias: true
head_h_donors_head_gain: 1.55
head_h_donors_head_dropout: 0.2
head_h_donors_head_wd: true

head_h_acceptors_head_eq_has_internal_weights: false
head_h_acceptors_head_mlp_latent_dimensions: [64]
head_h_acceptors_head_mlp_nonlinearity: swiglu
head_h_acceptors_head_has_bias: true
head_h_acceptors_head_gain: 1.55
head_h_acceptors_head_dropout: 0.2
head_h_acceptors_head_wd: true

head_noise_target_eq_has_internal_weights: false
head_noise_target_mlp_latent_dimensions: [32]
head_noise_target_mlp_nonlinearity: swiglu
head_noise_target_has_bias: true
head_noise_target_gain: 1.55
head_noise_target_dampen: true

head_hybridization_eq_has_internal_weights: false
head_hybridization_mlp_latent_dimensions: [64]
head_hybridization_mlp_nonlinearity: swiglu
head_hybridization_has_bias: true
head_hybridization_gain: 1.55
head_hybridization_dropout: 0.2
head_hybridization_wd: true

head_chirality_eq_has_internal_weights: false
head_chirality_mlp_latent_dimensions: [64]
head_chirality_mlp_nonlinearity: swiglu
head_chirality_has_bias: true
head_chirality_gain: 1.55
head_chirality_dropout: 0.2
head_chirality_wd: true

head_is_aromatic_eq_has_internal_weights: false
head_is_aromatic_mlp_latent_dimensions: [64]
head_is_aromatic_mlp_nonlinearity: swiglu
head_is_aromatic_has_bias: true
head_is_aromatic_gain: 1.55
head_is_aromatic_dropout: 0.2
head_is_aromatic_wd: true

head_is_in_ring_eq_has_internal_weights: false
head_is_in_ring_mlp_latent_dimensions: [64]
head_is_in_ring_mlp_nonlinearity: swiglu
head_is_in_ring_has_bias: true
head_is_in_ring_gain: 1.55
head_is_in_ring_dropout: 0.2
head_is_in_ring_wd: true

head_group_eq_has_internal_weights: false
head_group_mlp_latent_dimensions: [64]
head_group_mlp_nonlinearity: swiglu
head_group_has_bias: true
head_group_gain: 1.55
head_group_dropout: 0.2
head_group_wd: true

head_period_eq_has_internal_weights: false
head_period_mlp_latent_dimensions: [64]
head_period_mlp_nonlinearity: swiglu
head_period_has_bias: true
head_period_gain: 1.55
head_period_dropout: 0.2
head_period_wd: true

head_bond_type_eq_has_internal_weights: false
head_bond_type_mlp_latent_dimensions: [64]
head_bond_type_mlp_nonlinearity: swiglu
head_bond_type_has_bias: true
head_bond_type_gain: 1.55
head_bond_type_dropout: 0.2
head_bond_type_wd: true

head_bond_stereo_eq_has_internal_weights: false
head_bond_stereo_mlp_latent_dimensions: [64]
head_bond_stereo_mlp_nonlinearity: swiglu
head_bond_stereo_has_bias: true
head_bond_stereo_gain: 1.55
head_bond_stereo_dropout: 0.2
head_bond_stereo_wd: true

head_is_conjugated_eq_has_internal_weights: false
head_is_conjugated_mlp_latent_dimensions: [64]
head_is_conjugated_mlp_nonlinearity: swiglu
head_is_conjugated_has_bias: true
head_is_conjugated_gain: 1.55
head_is_conjugated_dropout: 0.2
head_is_conjugated_wd: true

head_binned_dists_eq_has_internal_weights: false
head_binned_dists_mlp_latent_dimensions: [64]
head_binned_dists_mlp_nonlinearity: swiglu
head_binned_dists_has_bias: true
head_binned_dists_gain: 1.55
head_binned_dists_dropout: 0.2
head_binned_dists_wd: true

head_degree_eq_has_internal_weights: false
head_degree_mlp_latent_dimensions: [64]
head_degree_mlp_nonlinearity: swiglu
head_degree_has_bias: true
head_degree_gain: 1.55
head_degree_dropout: 0.2
head_degree_wd: true

head_formal_charge_eq_has_internal_weights: false
head_formal_charge_mlp_latent_dimensions: [64]
head_formal_charge_mlp_nonlinearity: swiglu
head_formal_charge_has_bias: true
head_formal_charge_gain: 1.55
head_formal_charge_dropout: 0.2
head_formal_charge_wd: true

head_numH_eq_has_internal_weights: false
head_numH_mlp_latent_dimensions: [64]
head_numH_mlp_nonlinearity: swiglu
head_numH_has_bias: true
head_numH_gain: 1.55
head_numH_dropout: 0.2
head_numH_wd: true

head_count_frags_eq_has_internal_weights: false
head_count_frags_mlp_latent_dimensions: [64]
head_count_frags_mlp_nonlinearity: swiglu
head_count_frags_has_bias: true
head_count_frags_gain: 1.55
head_count_frags_dropout: 0.2
head_count_frags_wd: true

head_fragmentdsIds_present_eq_has_internal_weights: false
head_fragmentdsIds_present_mlp_latent_dimensions: [64]
head_fragmentdsIds_present_mlp_nonlinearity: swiglu
head_fragmentdsIds_present_has_bias: true
head_fragmentdsIds_present_gain: 1.55
head_fragmentdsIds_present_dropout: 0.2
head_fragmentdsIds_present_wd: true

# --- D A T A S E T --- #
# dataset_mode: ensemble
dataset_list:
  - dataset: npz
    dataset_input: /storage_common/nobilm/pretrain_paper/PCQM4Mv2Dataset/single_batch
    # inmemory: false
    transforms: [source.data_transforms._frad_transforms.frad, geqtrain.train.transforms.edges_dropout] # callables that returns a pyg.Data obj, order provided matters
    key_mapping:
      coords: pos
      atomic_num: node_types
      graph_labels: homo_lumo_gap
      hybridization: hybridization
      chirality: chirality
      is_aromatic: is_aromatic
      is_in_ring: is_in_ring
      rotable_bonds: rotable_bonds
      dihedral_angles_degrees: dihedral_angles_degrees
      group: group
      period: period
      bond_stereo: bond_stereo
      bond_type: bond_type
      edge_index: edge_index
      is_conjugated: is_conjugated
      adj_matrix: adj_matrix
      h_donors: h_donors_head
      h_acceptors: h_acceptors_head
      degree: degree
      formal_charge: formal_charge
      numH: numH
      count_frags: count_frags
      fragmentdsIds_present: fragmentdsIds_present # https://discuss.pytorch.org/t/multi-label-classification-in-pytorch/905/45
      nsafe: nsafe
      homo_lumo_gap: homo_lumo_gap

validation_dataset_list:
  - validation_dataset: npz
    validation_dataset_input: /storage_common/nobilm/pretrain_paper/PCQM4Mv2Dataset/single_batch
    # inmemory: false
    transforms: [source.data_transforms._frad_transforms.no_noise]
    key_mapping:
      coords: pos
      atomic_num: node_types # blue is the npz kword, red is the kword in code
      graph_labels: homo_lumo_gap
      hybridization: hybridization
      chirality: chirality
      is_aromatic: is_aromatic
      is_in_ring: is_in_ring
      group: group
      period: period
      bond_stereo: bond_stereo
      bond_type: bond_type
      edge_index: edge_index
      is_conjugated: is_conjugated
      adj_matrix: adj_matrix
      rotable_bonds: rotable_bonds
      dihedral_angles_degrees: dihedral_angles_degrees
      h_donors: h_donors_head
      h_acceptors: h_acceptors_head
      degree: degree
      formal_charge: formal_charge
      numH: numH
      count_frags: count_frags
      fragmentdsIds_present: fragmentdsIds_present
      nsafe: nsafe
      homo_lumo_gap: homo_lumo_gap

# - register fields - #
node_fields:
  - hybridization
  - chirality
  - is_aromatic
  - is_in_ring
  - group
  - period
  - degree
  - formal_charge
  - numH

edge_fields: #! DO NOT PUT EDGE INDEX HERE
  - bond_stereo
  - bond_type
  - is_conjugated

graph_fields:
  - homo_lumo_gap
  - adj_matrix
  - rotable_bonds
  - dihedral_angles_degrees
  - h_donors_head
  - h_acceptors_head
  - count_frags
  - fragmentdsIds_present
  - nsafe

exclude_keys: [adj_matrix, dihedral_angles_degrees, rotable_bonds]
ignore_fields:
 - adj_matrix
 - dihedral_angles_degrees
 - rotable_bonds

graph_attributes:
  rotable_bonds:
    fixed: True
  dihedral_angles_degrees:
    fixed: True
  count_frags:
    num_types: 64
    min_value: 0
    max_value: 128

# Field Properties
num_types: 32 # embedding_dimensionality of node_types, it must be present even if node_types not used in fwd
node_attributes: # if fixed (N,) if not fixed (1,N); for coords: same with an additional dim at -1 always equal to 3
  node_types: # this kword must match the red kword in key_mapping
    fixed: true # if equal for each frame, if so they must not have the batch dim in the npz
  group:
    num_types: 128 # actual: 118
    embedding_dimensionality: 32 # cumulative: 32
    fixed: true
  period:
    num_types: 128 # actual: 118
    embedding_dimensionality: 32 # cumulative: 64
    fixed: true
  hybridization:
    num_types: 8 # actual: 3
    embedding_dimensionality: 16 # cumulative: 80
    fixed: true
  chirality:
    num_types: 16 # actual: 9
    embedding_dimensionality: 32 # cumulative: 112
    fixed: true
  is_aromatic:
    num_types: 4 # actual: 2
    embedding_dimensionality: 8 # cumulative: 120
    fixed: true
  is_in_ring:
    num_types: 4 # actual: 2
    embedding_dimensionality: 8 # cumulative: 128
    fixed: true
  degree:
    num_types: 8 # actual: 4
    embedding_dimensionality: 8
    fixed: true
  formal_charge:
    num_types: 8 # actual: 4
    embedding_dimensionality: 8
    fixed: true
  numH:
    num_types: 8 # actual: 4
    embedding_dimensionality: 8
    fixed: true
  adj_matrix:
    fixed: true

edge_attributes: # shape in npz: (1, E)
    bond_stereo:
        num_types: 8 # actual: 7
        embedding_dimensionality: 16 # cumulative: 16
        fixed: true
    bond_type:
        num_types: 8 # actual: 5
        embedding_dimensionality: 16 # cumulative: 32
        fixed: true
    is_conjugated:
        num_types: 4 # actual: 3
        embedding_dimensionality: 8 # # cumulative: 40
        fixed: true


# --- L O S S --- #
loss_coeffs: #! do not use label_smoothing
    - homo_lumo_gap: # in here only 3 elements are allowed
      - MSELoss
    - noise_target:
      - MSELoss
    - hybridization:
      - CrossEntropyLoss
    - chirality:
      - CrossEntropyLoss
    - is_aromatic:
      - CrossEntropyLoss
    - is_in_ring:
      - CrossEntropyLoss
    - group:
      - CrossEntropyLoss
    - period:
      - CrossEntropyLoss
    - bond_type:
      - CrossEntropyLoss
    - bond_stereo:
      - CrossEntropyLoss
    - is_conjugated:
      - CrossEntropyLoss
    - h_donors_head:
      - CrossEntropyLoss
    - h_acceptors_head:
      - CrossEntropyLoss
    - degree:
      - CrossEntropyLoss
    - formal_charge:
      - CrossEntropyLoss
    - numH:
      - CrossEntropyLoss
    - binned_dists:
      - source.utils.tgt_utils.DiscreteDistLoss
      - rmax: 15.0 # this will be passed as k:v to params
        resolution: .1 # remember that this val is also related to the injected noise
    - count_frags:
      - CrossEntropyLoss
    - fragmentdsIds_present:
      - BCEWithLogitsLoss
    - nsafe:
      - CrossEntropyLoss

# --- M E T R I C S --- #
metrics_components:
    - homo_lumo_gap:
      - L1Loss
    - hybridization:
      - CrossEntropyLoss
    - chirality:
      - CrossEntropyLoss
    - is_aromatic:
      - CrossEntropyLoss
    - is_in_ring:
      - CrossEntropyLoss
    - group:
      - CrossEntropyLoss
    - period:
      - CrossEntropyLoss
    - bond_type:
      - CrossEntropyLoss
    - bond_stereo:
      - CrossEntropyLoss
    - is_conjugated:
      - CrossEntropyLoss
    - h_donors_head:
      - CrossEntropyLoss
    - h_acceptors_head:
      - CrossEntropyLoss
    - degree:
      - CrossEntropyLoss
    - formal_charge:
      - CrossEntropyLoss
    - numH:
      - CrossEntropyLoss
    - binned_dists:
      - source.utils.tgt_utils.DiscreteDistLoss
      - rmax: 15.0 # this will be passed as k:v to params
        resolution: .1
    - count_frags:
      - CrossEntropyLoss
    - fragmentdsIds_present:
      - BCEWithLogitsLoss
    - nsafe:
      - CrossEntropyLoss

# --- T R A I N I N G --- #
accumulation_steps: 4
batch_size: 64
validation_batch_size: 128
dataset_num_workers: 2 #20
train_dloader_n_workers: 2
val_dloader_n_workers: 2

max_epochs: 500
learning_rate: 2.e-5
metrics_key: validation_homo_lumo_gap_L1Loss_mean
# metric_criteria: increasing

# learning rate scheduler
warmup_epochs: 15%
lr_scheduler_name: CosineAnnealingLR

# lr_scheduler_name: ReduceLROnPlateau
# lr_scheduler_patience: 20
# lr_scheduler_factor: 0.75
# lr_scheduler_min_lr: 1.e-7

# early_stopping_lower_bounds:
#   LR: 1.e-6

early_stopping_patiences:
  validation_loss: 10


###################
default_dtype: float32
append: true # append (bool): if True, append the old model files and append the same logfile
debug: false

# - cutoffs - #
# avg_num_neighbors: 50.639081131

# - radial basis - #
edge_radial_attrs_basis: geqtrain.nn.BesselBasisVec
TanhCutoff_n: 6

# - symmetry - #
parity: o3_full

# --- interaction layers --- #
gnn_layers: 2 # this includes the final interaction block
num_layers: 2 # per interaction block
env_embed_multiplicity: 32

two_body_latent: geqtrain.nn.ScalarMLPFunction
two_body_latent_mlp_latent_dimensions: [128, 256, 512]
two_body_latent_mlp_nonlinearity: swiglu
two_body_latent_has_bias: true
two_body_latent_gain: 1.55

latent: geqtrain.nn.ScalarMLPFunction
latent_mlp_latent_dimensions: [256, 256, 256]
latent_mlp_nonlinearity: swiglu
latent_has_bias: true
latent_gain: 1.55

env_embed: geqtrain.nn.ScalarMLPFunction
env_embed_mlp_latent_dimensions: [256, 256, 256]
env_embed_mlp_nonlinearity: swiglu
env_embed_has_bias: true
env_embed_gain: 1.55

# --- update attrs --- #
update_mlp_latent_dimensions: [256, 256, 256]
update_mlp_nonlinearity: swiglu
update_has_bias: true
update_wd: true
update_gain: 1.55

update_emb_mlp_latent_dimensions: [256, 256, 256]
update_emb_mlp_nonlinearity: swiglu
update_emb_has_bias: true
update_emb_wd: true
update_emb_gain: 1.55

update_0_mlp_latent_dimensions: [256, 256, 256]
update_0_mlp_nonlinearity: swiglu
update_0_has_bias: true
update_0_wd: true
update_0_gain: 1.55

last_interaction_layer_output_ls: [0, 1]

# logging
verbose: info
wandb: true
wandb_watch: true # - log gradients and/or parameters of the model - #
code_folder_name: source # - use if you run geqtrain from a different repo, to save source code of your repo - #
wandb_watch_kwargs:
    log: 'gradients' # 'gradients', 'parameters', 'all', comment whole wandb_watch_kwargs to do not log any of previous
    log_freq: 1000  # upload log every N batches

# optimizer
optimizer_name: AdamW
optimizer_params:
  amsgrad: false
  betas: !!python/tuple
  - 0.9
  - 0.999
  eps: 1.0e-08 # do not change this
  weight_decay: 0
  fused: true

# Configure maximum batch sizes to avoid GPU memory errors. These parameters have to be configured according to your GPU RAM
skip_chunking: true
batch_max_atoms: 1000000             # Limit the maximum number of nodes of a graph to be loaded on memory in a single batch

dloader_timeout: 0
# dataloader_num_workers: 2
dloader_prefetch_factor: 4

train_val_split: random
shuffle: true
report_init_validation: false
